{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://movies.yahoo.com.tw/moviegenre_result.html?genre_id=1&page=1\n",
      "https://movies.yahoo.com.tw/moviegenre_result.html?genre_id=1&page=2\n",
      "https://movies.yahoo.com.tw/moviegenre_result.html?genre_id=1&page=3\n",
      "https://movies.yahoo.com.tw/moviegenre_result.html?genre_id=1&page=4\n",
      "https://movies.yahoo.com.tw/moviegenre_result.html?genre_id=1&page=5\n",
      "https://movies.yahoo.com.tw/moviegenre_result.html?genre_id=1&page=6\n",
      "https://movies.yahoo.com.tw/moviegenre_result.html?genre_id=1&page=7\n",
      "https://movies.yahoo.com.tw/moviegenre_result.html?genre_id=1&page=8\n",
      "https://movies.yahoo.com.tw/moviegenre_result.html?genre_id=1&page=9\n"
     ]
    }
   ],
   "source": [
    "import copy;\n",
    "import requests;\n",
    "from bs4 import BeautifulSoup;\n",
    "\n",
    "def getSoup(inurl):\n",
    "    r = requests.get(inurl)\n",
    "    r.encoding = 'utf-8'\n",
    "    soup=BeautifulSoup(r.text);\n",
    "    return soup;\n",
    "#end of getSoup\n",
    "\n",
    "def gen_page_number_list():\n",
    "    result=list();\n",
    "    magical_number=21;#this is found by trying manually\n",
    "    for i in range(1,magical_number+1):\n",
    "        sup=getSoup(\"https://movies.yahoo.com.tw/moviegenre_result.html?genre_id=\"+str(i)+\"&page=1\");\n",
    "        page_count=1;\n",
    "        if sup.find(\"div\",class_=\"page_numbox\") is not None:\n",
    "            lis=sup.find(\"div\",class_=\"page_numbox\").find_all(\"li\");\n",
    "            page_count=int(lis[len(lis)-3].text);\n",
    "        #print({\"id\":i,\"page_count\":page_count});\n",
    "        result.append({\"genre_id\":i,\"page_count\":page_count});\n",
    "    return result;\n",
    "#end of gen_page_number_list\n",
    "\n",
    "def extend_page_dataset(indataset,inurl):\n",
    "    if inurl in indataset[\"links\"]:\n",
    "        print(\"repeat\"+inurl+\"\\n\")\n",
    "        return indataset;\n",
    "    \n",
    "    a_data={\n",
    "        \"name\":\"\",\n",
    "        \"link\":\"\",\n",
    "        \"when\":\"\",\n",
    "        \"desc\":\"\",\n",
    "        \"class\":[]\n",
    "    };\n",
    "    sup=getSoup(inurl);\n",
    "    info_part=sup.find(\"div\",class_=\"movie_intro_info_r\");\n",
    "    desc_str=sup.find(\"span\",id=\"story\").text.strip();\n",
    "    \n",
    "    when_str=\"\";\n",
    "    for entry in info_part.find_all(\"span\"):\n",
    "        if entry.text.startswith(\"上映日期：\"):\n",
    "            when_str=copy.deepcopy(entry.text);\n",
    "            when_str=when_str[len(\"上映日期：\"):];\n",
    "            break;\n",
    "    if when_str==\"\":\n",
    "        raise Exception(\"date not found\");\n",
    "        \n",
    "    class_list=[];\n",
    "    for entry in info_part.find(\"div\",class_=\"level_name_box\").find_all(\"a\"):\n",
    "        class_list.append(entry.text.strip());\n",
    "    \n",
    "    \n",
    "    a_data[\"name\"]=info_part.find(\"h1\").text;\n",
    "    a_data[\"link\"]=inurl;\n",
    "    a_data[\"when\"]=when_str;\n",
    "    a_data[\"desc\"]=desc_str;\n",
    "    a_data[\"class\"]=class_list;\n",
    "    \n",
    "    indataset[\"data\"].append(a_data);\n",
    "    indataset[\"links\"].append(inurl);\n",
    "    return indataset;\n",
    "#end of extend_page_dataset\n",
    "\n",
    "\n",
    "def get_link_of_pages(inurl):\n",
    "    result=[];\n",
    "    sup=getSoup(inurl);\n",
    "    lis=sup.find(\"ul\",class_=\"release_list\").find_all(\"li\");\n",
    "    for entry in lis:\n",
    "        url=entry.find(\"a\",class_=\"gabtn\")[\"href\"];\n",
    "        result.append(url);\n",
    "    return result;\n",
    "#end of get_link_of_pages\n",
    "\n",
    "def homework_2():\n",
    "    dataset={\n",
    "        \"data\":[],\n",
    "        \"links\":[]\n",
    "    };\n",
    "    for entry in gen_page_number_list()[0:1]:\n",
    "        for pc in range(1,entry[\"page_count\"]-117):\n",
    "            url=\"https://movies.yahoo.com.tw/moviegenre_result.html?\"\n",
    "            url+=\"genre_id=\"+str(entry[\"genre_id\"]);\n",
    "            url+=\"&page=\"+str(pc);\n",
    "            for info_url in get_link_of_pages(url):\n",
    "                extend_page_dataset(dataset,info_url);\n",
    "            print(url);\n",
    "            if len(dataset[\"links\"])>6000:\n",
    "                break;\n",
    "        if len(dataset[\"links\"])>6000:\n",
    "                break;\n",
    "    return dataset;\n",
    "#end of homework_2\n",
    "\n",
    "\n",
    "things=homework_2();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "print(len(things[\"links\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
